{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/srinjay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '.', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'Hey', 'you', 'are', 'beautiful']\n",
      "[\"At eight o'clock on Thursday morning.\", \"Arthur didn't feel very good.\", 'Hey you are beautiful']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"At eight o'clock on Thursday morning. Arthur didn't feel very good. Hey you are beautiful \"\n",
    "token_word=nltk.word_tokenize(sentence)\n",
    "token_sent=nltk.sent_tokenize(sentence)\n",
    "print(token_word)\n",
    "print(token_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'under', \"won't\", 've', 'as', 'he', 'don', 'up', 'hadn', \"you'd\", 'doing', 'ourselves', 'other', 'they', 'needn', \"wasn't\", 'do', 'd', 'wasn', 'than', 'between', 'be', 'or', 'doesn', 'o', \"wouldn't\", 'with', 'over', 'at', \"don't\", 'while', \"mightn't\", 're', 'our', 'have', 'which', 'myself', 'just', 'was', 'won', \"weren't\", \"haven't\", 'ma', 'him', \"she's\", 'same', 'through', \"isn't\", 'we', 'some', 'm', 'against', 'because', 'were', 'couldn', 'very', \"you've\", 't', 'nor', 'himself', 'where', \"didn't\", 'i', 'below', 'it', 'who', 'weren', 'about', \"mustn't\", 'your', 'being', 'in', 'ain', 'until', \"you're\", 'by', 'can', 'should', 'theirs', 'aren', 'few', 'that', 'haven', \"shan't\", 'is', 'too', 'a', 'here', 'down', 'from', \"hadn't\", 'hasn', 'am', 'whom', 'into', 'when', 'so', \"hasn't\", 'each', 'not', 'his', 'mustn', 'then', 'an', 'there', 'if', 'again', 'my', 'the', 'most', 'yourselves', 'will', 'she', 'any', 'for', 'ours', 'both', \"needn't\", 'them', 'you', 'does', 'are', 'has', \"you'll\", 'these', 'y', 'above', 'themselves', 'only', 'their', \"shouldn't\", \"it's\", 'been', 'shan', 'during', \"couldn't\", \"that'll\", 's', 'own', 'on', 'didn', 'yourself', 'its', 'those', 'having', 'but', 'before', 'what', 'off', 'herself', 'such', 'of', 'more', 'to', 'itself', 'after', 'out', 'no', \"doesn't\", 'did', 'further', \"aren't\", 'all', 'had', 'this', 'shouldn', 'why', 'mightn', 'her', \"should've\", 'isn', 'll', 'and', 'once', 'wouldn', 'now', 'yours', 'hers', 'how', 'me'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sentence=\"Hey there, there are group fo people to tokenize. HEllo who areyou and what r u doing\"\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'there', ',', 'there', 'are', 'group', 'fo', 'people', 'to', 'tokenize', '.', 'HEllo', 'who', 'areyou', 'and', 'what', 'r', 'u', 'doing']\n"
     ]
    }
   ],
   "source": [
    "words=word_tokenize(example_sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'group', 'fo', 'people', 'tokenize', '.', 'HEllo', 'areyou', 'r', 'u']\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "for i in words:\n",
    "    if i not in stop_words:\n",
    "        x.append(i)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'group', 'fo', 'people', 'tokenize', '.', 'HEllo', 'areyou', 'r', 'u']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent=[w for w in words if not w in stop_words]\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bretle\n",
      "bretleeerwew\n",
      "erbretlee45\n",
      "vsvohsbretlee9949494323342\n",
      "blee575vdvl\n",
      "bretele\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps=PorterStemmer()\n",
    "example_words=[\"Bretlee\",\"Bretleeerwewe\",\"ErBretlee45\",\"vsvohsBretlee9949494323342\",\"Blee575vdvl\",\"Bretelee\",\"Pythonly\"]\n",
    "\n",
    "for w in  example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1119746/3950706476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mprocess_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1119746/3950706476.py\u001b[0m in \u001b[0;36mprocess_content\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChunkParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#print(chunked)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    POS taglist---\n",
    "\n",
    "    CC coordinating conjunction\n",
    "    CD cardinal digit\n",
    "    DT determiner\n",
    "    EX existential there (like: “there is” … think of it like “there exists”)\n",
    "    FW foreign word\n",
    "    IN preposition/subordinating conjunction\n",
    "    JJ adjective ‘big’\n",
    "    JJR adjective, comparative ‘bigger’\n",
    "    JJS adjective, superlative ‘biggest’\n",
    "    LS list marker 1)\n",
    "    MD modal could, will\n",
    "    NN noun, singular ‘desk’\n",
    "    NNS noun plural ‘desks’\n",
    "    NNP proper noun, singular ‘Harrison’\n",
    "    NNPS proper noun, plural ‘Americans’\n",
    "    PDT predeterminer ‘all the kids’\n",
    "    POS possessive ending parent’s\n",
    "    PRP personal pronoun I, he, she\n",
    "    PRP$ possessive pronoun my, his, hers\n",
    "    RB adverb very, silently,\n",
    "    RBR adverb, comparative better\n",
    "    RBS adverb, superlative best\n",
    "    RP particle give up\n",
    "    TO, to go ‘to’ the store.\n",
    "    UH interjection, errrrrrrrm\n",
    "    VB verb, base form take\n",
    "    VBD verb, past tense took\n",
    "    VBG verb, gerund/present participle taking\n",
    "    VBN verb, past participle taken\n",
    "    VBP verb, sing. present, non-3d take\n",
    "    VBZ verb, 3rd person sing. present takes\n",
    "    WDT wh-determiner which\n",
    "    WP wh-pronoun who, what\n",
    "    WP$ possessive wh-pronoun whose\n",
    "    WRB wh-abverb where, when\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "train_text= state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text=state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenize=PunktSentenceTokenizer(train_text)\n",
    "tokenized=custom_sent_tokenize.tokenize(sample_text)\n",
    "\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            #print(tagged)\n",
    "            chunkGram=r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            ChunkParser=nltk.RegexpParser(chunkGram)\n",
    "            chunked=ChunkParser.parse(tagged)\n",
    "            #print(chunked)\n",
    "            chunked.draw()\n",
    "\n",
    "    except Exception as e :\n",
    "        print(str(e))    \n",
    "\n",
    "process_content()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cebbce8138bc0cc82a262e3eb3473f0020e31c8f84efb858f2e6e737ab2ef2c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
